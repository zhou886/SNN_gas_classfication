<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial 3 - A Feedforward Spiking Neural Network &mdash; snntorch 0.6.2 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/default.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorial 4 - 2nd Order Spiking Neuron Models" href="tutorial_4.html" />
    <link rel="prev" title="Tutorial 2 - The Leaky Integrate-and-Fire Neuron" href="tutorial_2.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            snntorch
              <img src="../_static/snntorch_alpha_full.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.6.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../readme.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../snntorch.html">snntorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../snntorch.backprop.html">snntorch.backprop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../snntorch.functional.html">snntorch.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../snntorch.spikegen.html">snntorch.spikegen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../snntorch.spikeplot.html">snntorch.spikeplot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../snntorch.spikevision.html">snntorch.spikevision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../snntorch.surrogate.html">snntorch.surrogate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../snntorch.utils.html">snntorch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tutorial_1.html">Tutorial 1 - Spike Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_2.html">Tutorial 2 - The Leaky Integrate-and-Fire Neuron</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tutorial 3 - A Feedforward Spiking Neural Network</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#simplifying-the-leaky-integrate-and-fire-neuron-model">1. Simplifying the Leaky Integrate-and-Fire Neuron Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-decay-rate-beta">1.1 The Decay Rate: beta</a></li>
<li class="toctree-l4"><a class="reference internal" href="#weighted-input-current">1.2 Weighted Input Current</a></li>
<li class="toctree-l4"><a class="reference internal" href="#spiking-and-reset">1.3 Spiking and Reset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#code-implementation">1.4 Code Implementation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#leaky-neuron-model-in-snntorch">2. Leaky Neuron Model in snnTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#a-feedforward-spiking-neural-network">3. A Feedforward Spiking Neural Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_4.html">Tutorial 4 - 2nd Order Spiking Neuron Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_5.html">Tutorial 5 - Training Spiking Neural Networks with snntorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6.html">Tutorial 6 - Surrogate Gradient Descent in a Convolutional SNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_7.html">Tutorial 7 - Neuromorphic Datasets with Tonic + snnTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_ipu_1.html">Accelerating snnTorch on IPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_pop.html">Population Coding in Spiking Neural Nets</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_regression_1.html">Regression with SNNs: Part I</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_regression_2.html">Regression with SNNs: Part II</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../history.html">History</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">snntorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Tutorials</a></li>
      <li class="breadcrumb-item active">Tutorial 3 - A Feedforward Spiking Neural Network</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/tutorial_3.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tutorial-3-a-feedforward-spiking-neural-network">
<h1>Tutorial 3 - A Feedforward Spiking Neural Network<a class="headerlink" href="#tutorial-3-a-feedforward-spiking-neural-network" title="Permalink to this headline"></a></h1>
<p>Tutorial written by Jason K. Eshraghian (<a class="reference external" href="https://www.ncg.ucsc.edu">www.ncg.ucsc.edu</a>)</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_3_feedforward_snn.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a>
<p>The snnTorch tutorial series is based on the following paper. If you find these resources or code useful in your work, please consider citing the following source:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/2109.12894">Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish
Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. “Training
Spiking Neural Networks Using Lessons From Deep Learning”. arXiv preprint arXiv:2109.12894,
September 2021.</a></p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl class="simple">
<dt>This tutorial is a static non-editable version. Interactive, editable versions are available via the following links:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_3_feedforward_snn.ipynb">Google Colab</a></p></li>
<li><p><a class="reference external" href="https://github.com/jeshraghian/snntorch/tree/master/examples">Local Notebook (download via GitHub)</a></p></li>
</ul>
</dd>
</dl>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>In this tutorial, you will:</p>
<ul class="simple">
<li><p>Learn how to simplify the leaky integrate-and-fire (LIF) neuron to make it deep learning-friendly</p></li>
<li><p>Implement a feedforward spiking neural network (SNN)</p></li>
</ul>
<p>Install the latest PyPi distribution of snnTorch:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ pip install snntorch
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># imports</span>
<span class="kn">import</span> <span class="nn">snntorch</span> <span class="k">as</span> <span class="nn">snn</span>
<span class="kn">from</span> <span class="nn">snntorch</span> <span class="kn">import</span> <span class="n">spikeplot</span> <span class="k">as</span> <span class="n">splt</span>
<span class="kn">from</span> <span class="nn">snntorch</span> <span class="kn">import</span> <span class="n">spikegen</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</section>
<section id="simplifying-the-leaky-integrate-and-fire-neuron-model">
<h2>1. Simplifying the Leaky Integrate-and-Fire Neuron Model<a class="headerlink" href="#simplifying-the-leaky-integrate-and-fire-neuron-model" title="Permalink to this headline"></a></h2>
<p>In the previous tutorial, we designed our own LIF neuron model. But it was quite complex, and added an array of
hyperparameters to tune, including <span class="math notranslate nohighlight">\(R\)</span>, <span class="math notranslate nohighlight">\(C\)</span>,
<span class="math notranslate nohighlight">\(\Delta t\)</span>, <span class="math notranslate nohighlight">\(U_{\rm thr}\)</span>, and the choice of reset
mechanism. This is a lot to keep track of, and only grows more cumbersome
when scaled up to full-blown SNN. So let’s make a few
simplfications.</p>
<section id="the-decay-rate-beta">
<h3>1.1 The Decay Rate: beta<a class="headerlink" href="#the-decay-rate-beta" title="Permalink to this headline"></a></h3>
<p>In the previous tutorial, the Euler method was used to derive the
following solution to the passive membrane model:</p>
<div class="math notranslate nohighlight">
\[U(t+\Delta t) = (1-\frac{\Delta t}{\tau})U(t) + \frac{\Delta t}{\tau} I_{\rm in}(t)R \tag{1}\]</div>
<p>Now assume there is no input current, <span class="math notranslate nohighlight">\(I_{\rm in}(t)=0 A\)</span>:</p>
<div class="math notranslate nohighlight">
\[U(t+\Delta t) = (1-\frac{\Delta t}{\tau})U(t) \tag{2}\]</div>
<p>Let the ratio of subsequent values of <span class="math notranslate nohighlight">\(U\)</span>, i.e.,
<span class="math notranslate nohighlight">\(U(t+\Delta t)/U(t)\)</span> be the decay rate of the membrane potential,
also known as the inverse time constant:</p>
<div class="math notranslate nohighlight">
\[U(t+\Delta t) = \beta U(t) \tag{3}\]</div>
<p>From <span class="math notranslate nohighlight">\((1)\)</span>, this implies that:</p>
<div class="math notranslate nohighlight">
\[\beta = (1-\frac{\Delta t}{\tau}) \tag{4}\]</div>
<p>For reasonable accuracy, <span class="math notranslate nohighlight">\(\Delta t &lt;&lt; \tau\)</span>.</p>
</section>
<section id="weighted-input-current">
<h3>1.2 Weighted Input Current<a class="headerlink" href="#weighted-input-current" title="Permalink to this headline"></a></h3>
<p>If we assume <span class="math notranslate nohighlight">\(t\)</span> represents time-steps in a sequence rather than
continuous time, then we can set <span class="math notranslate nohighlight">\(\Delta t = 1\)</span>. To
further reduce the number of hyperparameters, assume <span class="math notranslate nohighlight">\(R=1\)</span>. From
<span class="math notranslate nohighlight">\((4)\)</span>, these assumptions lead to:</p>
<div class="math notranslate nohighlight">
\[\beta = (1-\frac{1}{C}) \implies (1-\beta)I_{\rm in} = \frac{1}{\tau}I_{\rm in} \tag{5}\]</div>
<p>The input current is weighted by <span class="math notranslate nohighlight">\((1-\beta)\)</span>.
By additionally assuming input current instantaneously contributes to the membrane potential:</p>
<div class="math notranslate nohighlight">
\[U[t+1] = \beta U[t] + (1-\beta)I_{\rm in}[t+1] \tag{6}\]</div>
<p>Note that the discretization of time means we are assuming that each
time bin <span class="math notranslate nohighlight">\(t\)</span> is brief enough such that a neuron may only emit a
maximum of one spike in this interval.</p>
<p>In deep learning, the weighting factor of an input is often a learnable
parameter. Taking a step away from the physically viable assumptions
made thus far, we subsume the effect of <span class="math notranslate nohighlight">\((1-\beta)\)</span> from
<span class="math notranslate nohighlight">\((6)\)</span> into a learnable weight <span class="math notranslate nohighlight">\(W\)</span>, and replace
<span class="math notranslate nohighlight">\(I_{\rm in}[t]\)</span> accordingly with an input <span class="math notranslate nohighlight">\(X[t]\)</span>:</p>
<div class="math notranslate nohighlight">
\[WX[t] = I_{\rm in}[t] \tag{7}\]</div>
<p>This can be interpreted in the following way. <span class="math notranslate nohighlight">\(X[t]\)</span> is an input
voltage, or spike, and is scaled by the synaptic conductance of
<span class="math notranslate nohighlight">\(W\)</span> to generate a current injection to the neuron. This gives us
the following result:</p>
<div class="math notranslate nohighlight">
\[U[t+1] = \beta U[t] + WX[t+1] \tag{8}\]</div>
<p>In future simulations, the effects of <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are decoupled.
<span class="math notranslate nohighlight">\(W\)</span> is a learnable parameter that is updated independently of <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
</section>
<section id="spiking-and-reset">
<h3>1.3 Spiking and Reset<a class="headerlink" href="#spiking-and-reset" title="Permalink to this headline"></a></h3>
<p>We now introduce the spiking and reset mechanisms. Recall that if
the membrane exceeds the threshold, then the neuron emits an output
spike:</p>
<div class="math notranslate nohighlight">
\[\begin{split}S[t] = \begin{cases} 1, &amp;\text{if}~U[t] &gt; U_{\rm thr} \\
0, &amp;\text{otherwise} \end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\tag{9}\]</div>
<p>If a spike is triggered, the membrane potential should be reset. The
<em>reset-by-subtraction</em> mechanism is modeled by:</p>
<div class="math notranslate nohighlight">
\[U[t+1] = \underbrace{\beta U[t]}_\text{decay} + \underbrace{WX[t+1]}_\text{input} - \underbrace{S[t]U_{\rm thr}}_\text{reset} \tag{10}\]</div>
<p>As <span class="math notranslate nohighlight">\(W\)</span> is a learnable parameter, and <span class="math notranslate nohighlight">\(U_{\rm thr}\)</span> is often
just set to <span class="math notranslate nohighlight">\(1\)</span> (though can be tuned), this leaves the decay rate
<span class="math notranslate nohighlight">\(\beta\)</span> as the only hyperparameter left to be specified. This
completes the painful part of this tutorial.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some implementations might make slightly different assumptions.
E.g., <span class="math notranslate nohighlight">\(S[t] \rightarrow S[t+1]\)</span> in <span class="math notranslate nohighlight">\((9)\)</span>, or
<span class="math notranslate nohighlight">\(X[t] \rightarrow X[t+1]\)</span> in <span class="math notranslate nohighlight">\((10)\)</span>. This above
derivation is what is used in snnTorch as we find it maps intuitively
to a recurrent neural network representation, without any change in
performance.</p>
</div>
</section>
<section id="code-implementation">
<h3>1.4 Code Implementation<a class="headerlink" href="#code-implementation" title="Permalink to this headline"></a></h3>
<p>Implementing this neuron in Python looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">leaky_integrate_and_fire</span><span class="p">(</span><span class="n">mem</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="n">spk</span> <span class="o">=</span> <span class="p">(</span><span class="n">mem</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span> <span class="c1"># if membrane exceeds threshold, spk=1, else, 0</span>
  <span class="n">mem</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">mem</span> <span class="o">+</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span> <span class="o">-</span> <span class="n">spk</span><span class="o">*</span><span class="n">threshold</span>
  <span class="k">return</span> <span class="n">spk</span><span class="p">,</span> <span class="n">mem</span>
</pre></div>
</div>
<p>To set <span class="math notranslate nohighlight">\(\beta\)</span>, we have the option of either using Equation
<span class="math notranslate nohighlight">\((3)\)</span> to define it, or hard-coding it directly. Here, we will use
<span class="math notranslate nohighlight">\((3)\)</span> for the sake of a demonstration, but in future, it will just be hard-coded as we
are more focused on something that works rather than biological precision.</p>
<p>Equation <span class="math notranslate nohighlight">\((3)\)</span> tells us that <span class="math notranslate nohighlight">\(\beta\)</span> is the ratio of
membrane potential across two subsequent time steps. Solve
this using the continuous time-dependent form of the equation (assuming
no current injection), which was derived in <a class="reference external" href="https://snntorch.readthedocs.io/en/latest/tutorials/index.html">Tutorial
2</a>:</p>
<div class="math notranslate nohighlight">
\[U(t) = U_0e^{-\frac{t}{\tau}}\]</div>
<p><span class="math notranslate nohighlight">\(U_0\)</span> is the initial membrane potential at <span class="math notranslate nohighlight">\(t=0\)</span>. Assume the
time-dependent equation is computed at discrete steps of
<span class="math notranslate nohighlight">\(t, (t+\Delta t), (t+2\Delta t)~...~\)</span>, then we can find the ratio
of membrane potential between subsequent steps using:</p>
<div class="math notranslate nohighlight">
\[\beta = \frac{U_0e^{-\frac{t+\Delta t}{\tau}}}{U_0e^{-\frac{t}{\tau}}} = \frac{U_0e^{-\frac{t + 2\Delta t}{\tau}}}{U_0e^{-\frac{t+\Delta t}{\tau}}} =~~...\]</div>
<div class="math notranslate nohighlight">
\[\implies \beta = e^{-\frac{\Delta t}{\tau}}\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># set neuronal parameters</span>
<span class="n">delta_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">tau</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5e-3</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">delta_t</span><span class="o">/</span><span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The decay rate is: </span><span class="si">{</span><span class="n">beta</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="go">The decay rate is: 0.819</span>
</pre></div>
</div>
<p>Run a quick simulation to check the neuron behaves correctly in
response to a step voltage input:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># initialize inputs/outputs + small step current input</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">190</span><span class="p">)</span><span class="o">*</span><span class="mf">0.5</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">mem</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">spk_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mem_rec</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">spk_rec</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># neuron parameters</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.819</span>

<span class="c1"># neuron simulation</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
  <span class="n">spk</span><span class="p">,</span> <span class="n">mem</span> <span class="o">=</span> <span class="n">leaky_integrate_and_fire</span><span class="p">(</span><span class="n">mem</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">step</span><span class="p">],</span> <span class="n">w</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
  <span class="n">mem_rec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mem</span><span class="p">)</span>
  <span class="n">spk_rec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spk</span><span class="p">)</span>

<span class="c1"># convert lists to tensors</span>
<span class="n">mem_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">mem_rec</span><span class="p">)</span>
<span class="n">spk_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">spk_rec</span><span class="p">)</span>

<span class="n">plot_cur_mem_spk</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">w</span><span class="p">,</span> <span class="n">mem_rec</span><span class="p">,</span> <span class="n">spk_rec</span><span class="p">,</span> <span class="n">thr_line</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ylim_max1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">title</span><span class="o">=</span><span class="s2">&quot;LIF Neuron Model With Weighted Step Voltage&quot;</span><span class="p">)</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/lif_step.png?raw=true"><img alt="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/lif_step.png?raw=true" class="align-center" src="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/lif_step.png?raw=true" style="width: 400px;" /></a>
</section>
</section>
<section id="leaky-neuron-model-in-snntorch">
<h2>2. Leaky Neuron Model in snnTorch<a class="headerlink" href="#leaky-neuron-model-in-snntorch" title="Permalink to this headline"></a></h2>
<p>This same thing can be achieved by instantiating <code class="docutils literal notranslate"><span class="pre">snn.Leaky</span></code>, in a
similar way to how we used <code class="docutils literal notranslate"><span class="pre">snn.Lapicque</span></code> in the previous tutorial, but with less hyperparameters:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lif1</span> <span class="o">=</span> <span class="n">snn</span><span class="o">.</span><span class="n">Leaky</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>
</div>
<p>The neuron model is now stored in <code class="docutils literal notranslate"><span class="pre">lif1</span></code>. To use this neuron:</p>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cur_in</span></code>: each element of <span class="math notranslate nohighlight">\(W\times X[t]\)</span> is sequentially passed as an input</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mem</span></code>: the previous step membrane potential, <span class="math notranslate nohighlight">\(U[t-1]\)</span>, is also passed as input.</p></li>
</ul>
<p><strong>Outputs</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">spk_out</span></code>: output spike <span class="math notranslate nohighlight">\(S[t]\)</span> (‘1’ if there is a spike; ‘0’ if there is no spike)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mem</span></code>: membrane potential <span class="math notranslate nohighlight">\(U[t]\)</span> of the present step</p></li>
</ul>
<p>These all need to be of type <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>. Note that here, we assume
the input current has already been weighted before passing into the
<code class="docutils literal notranslate"><span class="pre">snn.Leaky</span></code> neuron. This will make more sense when we construct a
network-scale model. Also, equation <span class="math notranslate nohighlight">\((10)\)</span> has been time-shifted
back one step without loss of generality.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Small step current input</span>
<span class="n">w</span><span class="o">=</span><span class="mf">0.21</span>
<span class="n">cur_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">190</span><span class="p">)</span><span class="o">*</span><span class="n">w</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">mem</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">spk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mem_rec</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">spk_rec</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># neuron simulation</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
  <span class="n">spk</span><span class="p">,</span> <span class="n">mem</span> <span class="o">=</span> <span class="n">lif1</span><span class="p">(</span><span class="n">cur_in</span><span class="p">[</span><span class="n">step</span><span class="p">],</span> <span class="n">mem</span><span class="p">)</span>
  <span class="n">mem_rec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mem</span><span class="p">)</span>
  <span class="n">spk_rec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spk</span><span class="p">)</span>

<span class="c1"># convert lists to tensors</span>
<span class="n">mem_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">mem_rec</span><span class="p">)</span>
<span class="n">spk_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">spk_rec</span><span class="p">)</span>

<span class="n">plot_cur_mem_spk</span><span class="p">(</span><span class="n">cur_in</span><span class="p">,</span> <span class="n">mem_rec</span><span class="p">,</span> <span class="n">spk_rec</span><span class="p">,</span> <span class="n">thr_line</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ylim_max1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">title</span><span class="o">=</span><span class="s2">&quot;snn.Leaky Neuron Model&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Compare this plot with the manually derived leaky integrate-and-fire neuron.
The membrane potential reset is slightly weaker: i.e., it uses a <em>soft reset</em>.
This has been done intentionally because it enables better performance on a few deep learning benchmarks.
The equation used instead is:</p>
<div class="math notranslate nohighlight">
\[U[t+1] = \underbrace{\beta U[t]}_\text{decay} + \underbrace{WX[t+1]}_\text{input} - \underbrace{\beta S[t]U_{\rm thr}}_\text{soft reset} \tag{11}\]</div>
<p>This model has the same optional input arguments of <code class="docutils literal notranslate"><span class="pre">reset_mechanism</span></code>
and <code class="docutils literal notranslate"><span class="pre">threshold</span></code> as described for Lapicque’s neuron model.</p>
<a class="reference internal image-reference" href="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/snn.leaky_step.png?raw=true"><img alt="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/snn.leaky_step.png?raw=true" class="align-center" src="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/snn.leaky_step.png?raw=true" style="width: 450px;" /></a>
</section>
<section id="a-feedforward-spiking-neural-network">
<h2>3. A Feedforward Spiking Neural Network<a class="headerlink" href="#a-feedforward-spiking-neural-network" title="Permalink to this headline"></a></h2>
<p>So far, we have only considered how a single neuron responds to input
stimulus. snnTorch makes it straightforward to scale this up to a deep
neural network. In this section, we will create a 3-layer fully-connected neural
network of dimensions 784-1000-10. Compared to our simulations so far, each neuron will now integrate over
many more incoming input spikes.</p>
<a class="reference internal image-reference" href="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/2_8_fcn.png?raw=true"><img alt="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/2_8_fcn.png?raw=true" class="align-center" src="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/2_8_fcn.png?raw=true" style="width: 600px;" /></a>
<p>PyTorch is used to form the connections between neurons, and
snnTorch to create the neurons. First, initialize all layers.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># layer parameters</span>
<span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">num_hidden</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">num_outputs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.99</span>

<span class="c1"># initialize layers</span>
<span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">)</span>
<span class="n">lif1</span> <span class="o">=</span> <span class="n">snn</span><span class="o">.</span><span class="n">Leaky</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
<span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">)</span>
<span class="n">lif2</span> <span class="o">=</span> <span class="n">snn</span><span class="o">.</span><span class="n">Leaky</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, initialize the hidden variables and outputs of each spiking
neuron. As networks increase in size, this becomes more tedious.
The static method <code class="docutils literal notranslate"><span class="pre">init_leaky()</span></code> can be used to take care of
this. All neurons in snnTorch have their own initialization methods that
follow this same syntax, e.g., <code class="docutils literal notranslate"><span class="pre">init_lapicque()</span></code>. The shape of the
hidden states are automatically initialized based on the input data
dimensions during the first forward pass.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize hidden states</span>
<span class="n">mem1</span> <span class="o">=</span> <span class="n">lif1</span><span class="o">.</span><span class="n">init_leaky</span><span class="p">()</span>
<span class="n">mem2</span> <span class="o">=</span> <span class="n">lif2</span><span class="o">.</span><span class="n">init_leaky</span><span class="p">()</span>

<span class="c1"># record outputs</span>
<span class="n">mem2_rec</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">spk1_rec</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">spk2_rec</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</div>
<p>Create an input spike train to pass to the network. There are 200 time
steps to simulate across 784 input neurons, i.e., the input originally
has dimensions of <span class="math notranslate nohighlight">\(200 \times 784\)</span>. However, neural nets typically process data in minibatches.
snnTorch, uses time-first dimensionality:</p>
<p>[<span class="math notranslate nohighlight">\(time \times batch\_size \times feature\_dimensions\)</span>]</p>
<p>So ‘unsqueeze’ the input along <code class="docutils literal notranslate"><span class="pre">dim=1</span></code> to indicate ‘one batch’
of data. The dimensions of this input tensor must be 200 <span class="math notranslate nohighlight">\(\times\)</span>
1 <span class="math notranslate nohighlight">\(\times\)</span> 784:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">spk_in</span> <span class="o">=</span> <span class="n">spikegen</span><span class="o">.</span><span class="n">rate_conv</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">200</span><span class="p">,</span> <span class="mi">784</span><span class="p">)))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dimensions of spk_in: </span><span class="si">{</span><span class="n">spk_in</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="s2">&quot;Dimensions of spk_in: torch.Size([200, 1, 784])&quot;</span>
</pre></div>
</div>
<p>Now it’s finally time to run a full simulation. An intuitive way to
think about how PyTorch and snnTorch work together is that PyTorch
routes the neurons together, and snnTorch loads the results into spiking
neuron models. In terms of coding up a network, these spiking neurons
can be treated like time-varying activation functions.</p>
<p>Here is a sequential account of what’s going on:</p>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(i^{th}\)</span> input from <code class="docutils literal notranslate"><span class="pre">spk_in</span></code> to the <span class="math notranslate nohighlight">\(j^{th}\)</span> neuron
is weighted by the parameters initialized in <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>:
<span class="math notranslate nohighlight">\(X_{i} \times W_{ij}\)</span></p></li>
<li><p>This generates the input current term from Equation <span class="math notranslate nohighlight">\((10)\)</span>,
contributing to <span class="math notranslate nohighlight">\(U[t+1]\)</span> of the spiking neuron</p></li>
<li><p>If <span class="math notranslate nohighlight">\(U[t+1] &gt; U_{\rm thr}\)</span>, then a spike is triggered from this
neuron</p></li>
<li><p>This spike is weighted by the second layer weight, and the above
process is repeated for all inputs, weights, and neurons.</p></li>
<li><p>If there is no spike, then nothing is passed to the post-synaptic
neuron.</p></li>
</ul>
<p>The only difference from our simulations thus far is that we are now
scaling the input current with a weight generated by <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>,
rather than manually setting <span class="math notranslate nohighlight">\(W\)</span> ourselves.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># network simulation</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">cur1</span> <span class="o">=</span> <span class="n">fc1</span><span class="p">(</span><span class="n">spk_in</span><span class="p">[</span><span class="n">step</span><span class="p">])</span> <span class="c1"># post-synaptic current &lt;-- spk_in x weight</span>
    <span class="n">spk1</span><span class="p">,</span> <span class="n">mem1</span> <span class="o">=</span> <span class="n">lif1</span><span class="p">(</span><span class="n">cur1</span><span class="p">,</span> <span class="n">mem1</span><span class="p">)</span> <span class="c1"># mem[t+1] &lt;--post-syn current + decayed membrane</span>
    <span class="n">cur2</span> <span class="o">=</span> <span class="n">fc2</span><span class="p">(</span><span class="n">spk1</span><span class="p">)</span>
    <span class="n">spk2</span><span class="p">,</span> <span class="n">mem2</span> <span class="o">=</span> <span class="n">lif2</span><span class="p">(</span><span class="n">cur2</span><span class="p">,</span> <span class="n">mem2</span><span class="p">)</span>

    <span class="n">mem2_rec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mem2</span><span class="p">)</span>
    <span class="n">spk1_rec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spk1</span><span class="p">)</span>
    <span class="n">spk2_rec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spk2</span><span class="p">)</span>

<span class="c1"># convert lists to tensors</span>
<span class="n">mem2_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">mem2_rec</span><span class="p">)</span>
<span class="n">spk1_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">spk1_rec</span><span class="p">)</span>
<span class="n">spk2_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">spk2_rec</span><span class="p">)</span>

<span class="n">plot_snn_spikes</span><span class="p">(</span><span class="n">spk_in</span><span class="p">,</span> <span class="n">spk1_rec</span><span class="p">,</span> <span class="n">spk2_rec</span><span class="p">,</span> <span class="s2">&quot;Fully Connected Spiking Neural Network&quot;</span><span class="p">)</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/mlp_raster.png?raw=true"><img alt="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/mlp_raster.png?raw=true" class="align-center" src="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/mlp_raster.png?raw=true" style="width: 450px;" /></a>
<p>At this stage, the spikes don’t have any real meaning. The inputs and
weights are all randomly initialized, and no training has taken place.
But the spikes should appear to be propagating from the first layer
through to the output. If you are not seeing any spikes, then you might have</p>
<blockquote>
<div><p>been unlucky in the weight initialization lottery - you might want</p>
</div></blockquote>
<p>to try re-running the last four code-blocks.</p>
<p><code class="docutils literal notranslate"><span class="pre">spikeplot.spike_count</span></code> can create a spike counter of
the output layer. The following animation will take some time to
generate.</p>
<blockquote>
<div><p>Note: if you are running the notebook locally on your desktop, please
uncomment the line below and modify the path to your ffmpeg.exe</p>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;2&#39;</span><span class="p">,</span> <span class="s1">&#39;3&#39;</span><span class="p">,</span> <span class="s1">&#39;4&#39;</span><span class="p">,</span> <span class="s1">&#39;5&#39;</span><span class="p">,</span> <span class="s1">&#39;6&#39;</span><span class="p">,</span> <span class="s1">&#39;7&#39;</span><span class="p">,</span> <span class="s1">&#39;8&#39;</span><span class="p">,</span><span class="s1">&#39;9&#39;</span><span class="p">]</span>
<span class="n">spk2_rec</span> <span class="o">=</span> <span class="n">spk2_rec</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

<span class="c1"># plt.rcParams[&#39;animation.ffmpeg_path&#39;] = &#39;C:\\path\\to\\your\\ffmpeg.exe&#39;</span>

<span class="c1">#  Plot spike count histogram</span>
<span class="n">anim</span> <span class="o">=</span> <span class="n">splt</span><span class="o">.</span><span class="n">spike_count</span><span class="p">(</span><span class="n">spk2_rec</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">animate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">anim</span><span class="o">.</span><span class="n">to_html5_video</span><span class="p">())</span>
<span class="c1"># anim.save(&quot;spike_bar.mp4&quot;)</span>
</pre></div>
</div>
<center>
  <video controls src="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/spike_bar.mp4?raw=true"></video>
</center><p><code class="docutils literal notranslate"><span class="pre">spikeplot.traces</span></code> lets you visualize the membrane potential traces. We will plot 9 out of 10 output neurons.
Compare it to the animation and raster plot above to see if you can match the traces to the neuron.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot membrane potential traces</span>
<span class="n">splt</span><span class="o">.</span><span class="n">traces</span><span class="p">(</span><span class="n">mem2_rec</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">spk</span><span class="o">=</span><span class="n">spk2_rec</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/traces.png?raw=true"><img alt="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/traces.png?raw=true" class="align-center" src="https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial3/_static/traces.png?raw=true" style="width: 450px;" /></a>
<p>It is fairly normal if some neurons are firing while others are
completely dead. Again, none of these spikes have any real meaning until
the weights have been trained.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline"></a></h2>
<p>That covers how to simplify the leaky integrate-and-fire neuron model,
and then using it to build a spiking neural network. In practice, we
will almost always prefer to use <code class="docutils literal notranslate"><span class="pre">snn.Leaky</span></code> over <code class="docutils literal notranslate"><span class="pre">snn.Lapicque</span></code> for
training networks, as there is a smaller hyperparameter search space.</p>
<p><a class="reference external" href="https://snntorch.readthedocs.io/en/latest/tutorials/index.html">Tutorial
4</a>
goes into detail with the 2nd-order <code class="docutils literal notranslate"><span class="pre">snn.Synaptic</span></code> and <code class="docutils literal notranslate"><span class="pre">snn.Alpha</span></code>
models. This next tutorial is not necessary for training a network, so if you wish to go straight
to deep learning with snnTorch, then skip ahead to <a class="reference external" href="https://snntorch.readthedocs.io/en/latest/tutorials/index.html">Tutorial
5</a>.</p>
<p>If you like this project, please consider starring ⭐ the repo on GitHub as it is the easiest and best way to support it.</p>
<p>For reference, the documentation <a class="reference external" href="https://snntorch.readthedocs.io/en/latest/snntorch.html">can be found
here</a>.</p>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/jeshraghian/snntorch">Check out the snnTorch GitHub project here.</a></p></li>
<li><p><a class="reference external" href="https://snntorch.readthedocs.io/en/latest/snntorch.html">snnTorch
documentation</a>
of the Lapicque, Leaky, Synaptic, and Alpha models</p></li>
<li><p><a class="reference external" href="https://neuronaldynamics.epfl.ch/index.html">Neuronal Dynamics: From single neurons to networks and models of
cognition</a> by Wulfram
Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski.</p></li>
<li><p><a class="reference external" href="https://mitpress.mit.edu/books/theoretical-neuroscience">Theoretical Neuroscience: Computational and Mathematical Modeling of
Neural
Systems</a>
by Laurence F. Abbott and Peter Dayan</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="tutorial_2.html" class="btn btn-neutral float-left" title="Tutorial 2 - The Leaky Integrate-and-Fire Neuron" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tutorial_4.html" class="btn btn-neutral float-right" title="Tutorial 4 - 2nd Order Spiking Neuron Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Jason K. Eshraghian.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>